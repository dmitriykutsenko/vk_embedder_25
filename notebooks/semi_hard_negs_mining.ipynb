{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3fd4a-c711-435c-8817-4de87d1b1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a2532-ad4f-47fa-ac9d-69f34fae222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"prefix_type\":          \"search\",\n",
    "    \"hard_neg_rank_start\":  10,\n",
    "    \"hard_neg_rank_end\":    50,\n",
    "    \"hard_neg_count\":       5,\n",
    "    \"max_triplets\":         100_000,\n",
    "    \"log_every\":            500,\n",
    "    \"max_tokens\":           1024,\n",
    "    \"margin_tau\":           0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45127e85-bede-45cf-9ef2-d26f69909b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepvk/USER2-base\")\n",
    "model = SentenceTransformer(\"deepvk/USER2-base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "def prompt_name(is_query: bool) -> str:\n",
    "    return \"search_query\" if CONFIG[\"prefix_type\"] == \"search\" and is_query else \"search_document\" if CONFIG[\"prefix_type\"] == \"search\" else \"clustering\"\n",
    "\n",
    "def create_triplets(split, q_field: str, p_field: str, max_tokens: int = 500, margin_tau: float = CONFIG['margin_tau']):\n",
    "    examples = [ex for ex in split if ex.get(q_field) and ex.get(p_field)]\n",
    "    random.shuffle(examples)\n",
    "    if not examples:\n",
    "        return []\n",
    "\n",
    "    unique_docs, doc2idx, ex2doc_idx = [], {}, {}\n",
    "    for ex_id, ex in enumerate(tqdm(examples)):\n",
    "        doc = ex[p_field]\n",
    "        if count_tokens(doc) > max_tokens:\n",
    "            continue\n",
    "        if doc not in doc2idx:\n",
    "            doc2idx[doc] = len(unique_docs)\n",
    "            unique_docs.append(doc)\n",
    "        ex2doc_idx[ex_id] = doc2idx[doc]\n",
    "\n",
    "    if not unique_docs:\n",
    "        return []\n",
    "\n",
    "    doc_embs = model.encode(unique_docs, prompt_name=prompt_name(False), convert_to_numpy=True, show_progress_bar=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(doc_embs)\n",
    "    index = faiss.IndexFlatIP(doc_embs.shape[1])\n",
    "    index.add(doc_embs)\n",
    "    triplets = []\n",
    "\n",
    "    for ex_id, ex in enumerate(tqdm(examples)):\n",
    "        if len(triplets) >= CONFIG[\"max_triplets\"]:\n",
    "            break\n",
    "        if ex_id and ex_id % CONFIG[\"log_every\"] == 0:\n",
    "            print(f\"{ex_id}/{len(examples)} â†’ {len(triplets)} triplets\")\n",
    "\n",
    "        q_text = ex[q_field]\n",
    "        if count_tokens(q_text) > max_tokens:\n",
    "            continue\n",
    "\n",
    "        pos_idx = ex2doc_idx.get(ex_id)\n",
    "        if pos_idx is None:\n",
    "            continue\n",
    "        pos_text = unique_docs[pos_idx]\n",
    "\n",
    "        q_emb = model.encode(q_text, prompt_name=prompt_name(True), convert_to_numpy=True, show_progress_bar=False).astype(\"float32\")\n",
    "        q_emb_2d = q_emb.reshape(1, -1)\n",
    "        faiss.normalize_L2(q_emb_2d)\n",
    "        q_emb = q_emb_2d[0]\n",
    "\n",
    "        pos_sim = float(np.dot(q_emb, doc_embs[pos_idx]))\n",
    "        _, neigh = index.search(q_emb_2d, CONFIG[\"hard_neg_rank_end\"] + 1)\n",
    "\n",
    "        candidates = [i for i in neigh[0][CONFIG[\"hard_neg_rank_start\"]:CONFIG[\"hard_neg_rank_end\"] + 1]\n",
    "                      if i != pos_idx and float(np.dot(q_emb, doc_embs[i])) < pos_sim - margin_tau]\n",
    "\n",
    "        random.shuffle(candidates)\n",
    "        neg_idxs = candidates[:CONFIG[\"hard_neg_count\"]]\n",
    "\n",
    "        if len(neg_idxs) < CONFIG[\"hard_neg_count\"]:\n",
    "            continue\n",
    "\n",
    "        triplets.append({\n",
    "            \"query\": q_text,\n",
    "            \"positive\": pos_text,\n",
    "            \"negatives\": [unique_docs[i] for i in neg_idxs]\n",
    "        })\n",
    "\n",
    "    unique_triplets = { (t[\"query\"], t[\"positive\"], tuple(t[\"negatives\"])): t for t in triplets }\n",
    "    return list(unique_triplets.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ff4ec-0d8d-4150-85b5-db5e12be52c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

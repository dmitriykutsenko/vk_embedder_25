{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3fd4a-c711-435c-8817-4de87d1b1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a2532-ad4f-47fa-ac9d-69f34fae222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"prefix_type\":          \"search\",\n",
    "    \"hard_neg_rank_start\":  10,\n",
    "    \"hard_neg_rank_end\":    50,\n",
    "    \"hard_neg_count\":       5,\n",
    "    \"max_triplets\":         100_000,\n",
    "    \"log_every\":            500,\n",
    "    \"max_tokens\":           1024,\n",
    "    \"margin_tau\":           0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50249892-ece2-4bc1-a654-dd263e05d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepvk/USER2-base\")\n",
    "model = SentenceTransformer(\"deepvk/USER2-base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "def prompt_name(is_query: bool) -> str:\n",
    "    return \"search_query\" if CONFIG[\"prefix_type\"] == \"search\" and is_query else \"search_document\" if CONFIG[\"prefix_type\"] == \"search\" else \"clustering\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45127e85-bede-45cf-9ef2-d26f69909b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(split, q_field: str, p_field: str, max_tokens: int = 500, margin_tau: float = CONFIG['margin_tau']):\n",
    "    examples = [ex for ex in split if ex.get(q_field) and ex.get(p_field)]\n",
    "    random.shuffle(examples)\n",
    "    if not examples:\n",
    "        return []\n",
    "\n",
    "    unique_docs, doc2idx, ex2doc_idx = [], {}, {}\n",
    "    for ex_id, ex in enumerate(tqdm(examples)):\n",
    "        doc = ex[p_field]\n",
    "        if count_tokens(doc) > max_tokens:\n",
    "            continue\n",
    "        if doc not in doc2idx:\n",
    "            doc2idx[doc] = len(unique_docs)\n",
    "            unique_docs.append(doc)\n",
    "        ex2doc_idx[ex_id] = doc2idx[doc]\n",
    "\n",
    "    if not unique_docs:\n",
    "        return []\n",
    "\n",
    "    doc_embs = model.encode(unique_docs, prompt_name=prompt_name(False), convert_to_numpy=True, show_progress_bar=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(doc_embs)\n",
    "    index = faiss.IndexFlatIP(doc_embs.shape[1])\n",
    "    index.add(doc_embs)\n",
    "    triplets = []\n",
    "\n",
    "    for ex_id, ex in enumerate(tqdm(examples)):\n",
    "        if len(triplets) >= CONFIG[\"max_triplets\"]:\n",
    "            break\n",
    "        if ex_id and ex_id % CONFIG[\"log_every\"] == 0:\n",
    "            print(f\"{ex_id}/{len(examples)} â†’ {len(triplets)} triplets\")\n",
    "\n",
    "        q_text = ex[q_field]\n",
    "        if count_tokens(q_text) > max_tokens:\n",
    "            continue\n",
    "\n",
    "        pos_idx = ex2doc_idx.get(ex_id)\n",
    "        if pos_idx is None:\n",
    "            continue\n",
    "        pos_text = unique_docs[pos_idx]\n",
    "\n",
    "        q_emb = model.encode(q_text, prompt_name=prompt_name(True), convert_to_numpy=True, show_progress_bar=False).astype(\"float32\")\n",
    "        q_emb_2d = q_emb.reshape(1, -1)\n",
    "        faiss.normalize_L2(q_emb_2d)\n",
    "        q_emb = q_emb_2d[0]\n",
    "\n",
    "        pos_sim = float(np.dot(q_emb, doc_embs[pos_idx]))\n",
    "        _, neigh = index.search(q_emb_2d, CONFIG[\"hard_neg_rank_end\"] + 1)\n",
    "\n",
    "        candidates = [i for i in neigh[0][CONFIG[\"hard_neg_rank_start\"]:CONFIG[\"hard_neg_rank_end\"] + 1]\n",
    "                      if i != pos_idx and float(np.dot(q_emb, doc_embs[i])) < pos_sim - margin_tau]\n",
    "\n",
    "        random.shuffle(candidates)\n",
    "        neg_idxs = candidates[:CONFIG[\"hard_neg_count\"]]\n",
    "\n",
    "        if len(neg_idxs) < CONFIG[\"hard_neg_count\"]:\n",
    "            continue\n",
    "\n",
    "        triplets.append({\n",
    "            \"query\": q_text,\n",
    "            \"positive\": pos_text,\n",
    "            \"negatives\": [unique_docs[i] for i in neg_idxs]\n",
    "        })\n",
    "\n",
    "    unique_triplets = { (t[\"query\"], t[\"positive\"], tuple(t[\"negatives\"])): t for t in triplets }\n",
    "    return list(unique_triplets.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3ea51-524f-4475-8589-c617b7543c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hard_negative_quality(triplets, sample_size=CONFIG['sample_size']):\n",
    "    sample = random.sample(triplets, k=min(sample_size, len(triplets)))\n",
    "\n",
    "    q_texts   = [t[\"query\"] for t in sample]\n",
    "    pos_texts = [t[\"positive\"] for t in sample]\n",
    "    neg_texts = [n for t in sample for n in t[\"negatives\"]]\n",
    "\n",
    "    q_embs = model.encode(q_texts, prompt_name=prompt_name(True), convert_to_numpy=True, show_progress_bar=True)\n",
    "    pos_embs = model.encode(pos_texts, prompt_name=prompt_name(False), convert_to_numpy=True, show_progress_bar=True)\n",
    "    neg_embs = model.encode(neg_texts, prompt_name=prompt_name(False), convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "    q_embs /= np.linalg.norm(q_embs, axis=1, keepdims=True)\n",
    "    pos_embs /= np.linalg.norm(pos_embs, axis=1, keepdims=True)\n",
    "    neg_embs /= np.linalg.norm(neg_embs, axis=1, keepdims=True)\n",
    "\n",
    "    pos_sims = (q_embs * pos_embs).sum(axis=1)\n",
    "\n",
    "    neg_sims, margins = [], []\n",
    "    idx, bad_cnt = 0, 0\n",
    "    for i, t in tqdm(enumerate(sample), total=len(sample)):\n",
    "        n = len(t[\"negatives\"])\n",
    "        sims = q_embs[i] @ neg_embs[idx:idx + n].T\n",
    "\n",
    "        hardest = sims.max()\n",
    "        neg_sims.extend(sims.tolist())\n",
    "\n",
    "        if hardest >= pos_sims[i]:\n",
    "            bad_cnt += 1\n",
    "        margins.append(pos_sims[i] - hardest)\n",
    "\n",
    "        idx += n\n",
    "\n",
    "    return {\n",
    "        \"sampled_triplets\": len(sample),\n",
    "        \"mean_pos_sim\": float(np.mean(pos_sims)),\n",
    "        \"mean_neg_sim\": float(np.mean(neg_sims)),\n",
    "        \"mean_margin_pos_vs_hardest\": float(np.mean(margins)),\n",
    "        \"triplets_with_harder_negative\": bad_cnt\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    triplets = [...] \n",
    "\n",
    "    if not triplets:\n",
    "        raise RuntimeError(\"Triplets list empty!\")\n",
    "\n",
    "    stats = evaluate_hard_negative_quality(triplets)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Sampled {stats['sampled_triplets']} triplets\n",
    "    Mean cos-sim(query, positive): {stats['mean_pos_sim']:.4f}\n",
    "    Mean cos-sim(query, negatives): {stats['mean_neg_sim']:.4f}\n",
    "    Mean margin (pos - hardest neg): {stats['mean_margin_pos_vs_hardest']:.4f}\n",
    "    Triplets with harder negative: {stats['triplets_with_harder_negative']}\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-29T19:51:59.841426Z",
     "iopub.status.busy": "2025-05-29T19:51:59.840885Z",
     "iopub.status.idle": "2025-05-29T19:53:17.885788Z",
     "shell.execute_reply": "2025-05-29T19:53:17.885076Z",
     "shell.execute_reply.started": "2025-05-29T19:51:59.841397Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install mteb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IDX_SLERP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:53:57.858478Z",
     "iopub.status.busy": "2025-05-29T19:53:57.857646Z",
     "iopub.status.idle": "2025-05-29T19:57:43.397817Z",
     "shell.execute_reply": "2025-05-29T19:57:43.397092Z",
     "shell.execute_reply.started": "2025-05-29T19:53:57.858446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from safetensors.torch import load_file, save_file\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer\n",
    "import mteb\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "\n",
    "symm_model_dir = \"/kaggle/input/s2-models-embed/epoch_1_model/epoch_1\"      \n",
    "asymm_model_dir = \"/kaggle/input/s2-models-embed/epoch_1_asym/epoch_1_asym\"     \n",
    "\n",
    "blend_weight = [0.1, 0.2, 0.3, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95][IDX_SLERP]                     \n",
    "output_merged = \"merged_model\"\n",
    "\n",
    "def slerp_weights(state_dict1, state_dict2, alpha):\n",
    "    merged = {}\n",
    "    \n",
    "    \n",
    "    if set(state_dict1.keys()) != set(state_dict2.keys()):\n",
    "        raise ValueError(\"Models have different architectures - key mismatch\")\n",
    "    \n",
    "    for k in state_dict1:\n",
    "        w1 = state_dict1[k].float()\n",
    "        w2 = state_dict2[k].float()\n",
    "        \n",
    "\n",
    "        if w1.shape != w2.shape:\n",
    "            print(f\"Warning: Shape mismatch for {k}: {w1.shape} vs {w2.shape}\")\n",
    "            merged[k] = w1  \n",
    "            continue\n",
    "            \n",
    "        \n",
    "        v1 = w1.view(-1)\n",
    "        v2 = w2.view(-1)\n",
    "        \n",
    "        dot = torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2) + 1e-8)\n",
    "        omega = torch.acos(torch.clamp(dot, -1.0, 1.0))\n",
    "        \n",
    "        if torch.abs(omega) < 1e-6:\n",
    "            merged_tensor = w1\n",
    "        else:\n",
    "            so = torch.sin(omega)\n",
    "            part1 = torch.sin((1 - alpha) * omega) / so\n",
    "            part2 = torch.sin(alpha * omega) / so\n",
    "            merged_flat = part1 * v1 + part2 * v2\n",
    "            merged_tensor = merged_flat.view_as(w1)\n",
    "            \n",
    "        merged[k] = merged_tensor\n",
    "    return merged\n",
    "\n",
    "os.makedirs(output_merged, exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "print(\"Loading model state dicts...\")\n",
    "\n",
    "sd_symm = load_file(f\"{symm_model_dir}/model.safetensors\")\n",
    "sd_asymm = load_file(f\"{asymm_model_dir}/model.safetensors\")\n",
    "\n",
    "print(\"Merging models with SLERP...\")\n",
    "alpha = blend_weight\n",
    "sd_merged = slerp_weights(sd_symm, sd_asymm, alpha)\n",
    "\n",
    "print(\"Saving merged model...\")\n",
    "save_file(sd_merged, f\"{output_merged}/model.safetensors\")\n",
    "\n",
    "config_files = ['config.json', 'tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json']\n",
    "for fname in config_files:\n",
    "    src = f\"{symm_model_dir}/{fname}\"\n",
    "    dst = f\"{output_merged}/{fname}\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"Copied {fname}\")\n",
    "    else:\n",
    "        print(f\"Warning: {fname} not found in {symm_model_dir}\")\n",
    "\n",
    "print(\"Building SentenceTransformer with merged weights...\")\n",
    "dir_merged = output_merged\n",
    "transformer = models.Transformer(\n",
    "    model_name_or_path=dir_merged,\n",
    "    tokenizer_name_or_path=dir_merged\n",
    ")\n",
    "pooling = models.Pooling(\n",
    "    transformer.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "base_model = SentenceTransformer(modules=[transformer, pooling])\n",
    "\n",
    "class PrefixSentenceTransformer:\n",
    "    def __init__(self, base_model, query_prefix=\"search_query:\", doc_prefix=\"search_document:\"):\n",
    "        self.base_model = base_model\n",
    "        self.query_prefix = query_prefix\n",
    "        self.doc_prefix = doc_prefix\n",
    "        \n",
    "    def encode(self, sentences, **kwargs):\n",
    "        is_query = kwargs.pop('is_query', False)\n",
    "        \n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "            \n",
    "        if is_query:\n",
    "            prefixed_sentences = [f\"{self.query_prefix} {sent}\" for sent in sentences]\n",
    "        else:\n",
    "            prefixed_sentences = [f\"{self.doc_prefix} {sent}\" for sent in sentences]\n",
    "            \n",
    "        return self.base_model.encode(prefixed_sentences, **kwargs)\n",
    "    \n",
    "    def encode_queries(self, queries, **kwargs):\n",
    "        return self.encode(queries, is_query=True, **kwargs)\n",
    "    \n",
    "    def encode_corpus(self, corpus, **kwargs):\n",
    "        if isinstance(corpus, dict) and len(corpus) > 0:\n",
    "            texts = []\n",
    "            for doc_id, doc in corpus.items():\n",
    "                if isinstance(doc, dict):\n",
    "                    title = doc.get('title', '')\n",
    "                    text = doc.get('text', '')\n",
    "                    combined = f\"{title} {text}\".strip() if title else text\n",
    "                    texts.append(combined)\n",
    "                else:\n",
    "                    texts.append(str(doc))\n",
    "            return self.encode(texts, is_query=False, **kwargs)\n",
    "        else:\n",
    "            return self.encode(corpus, is_query=False, **kwargs)\n",
    "    \n",
    "    def save(self, path, **kwargs):\n",
    "        return self.base_model.save(path, **kwargs)\n",
    "    \n",
    "    def push_to_hub(self, *args, **kwargs):\n",
    "        return self.base_model.push_to_hub(*args, **kwargs)\n",
    "\n",
    "wrapped = PrefixSentenceTransformer(base_model)\n",
    "\n",
    "print(\"Starting MTEB evaluation...\")\n",
    "tasks_list = [\n",
    "    'CEDRClassification', 'GeoreviewClassification',\n",
    "    'GeoreviewClusteringP2P', 'HeadlineClassification',\n",
    "    'InappropriatenessClassification', 'KinopoiskClassification', 'RUParaPhraserSTS',\n",
    "    'RuReviewsClassification','RuSTSBenchmarkSTS', 'RuSciBenchGRNTIClassification',\n",
    "    'RuSciBenchGRNTIClusteringP2P', 'RuSciBenchOECDClassification',\n",
    "    'RuSciBenchOECDClusteringP2P', 'SensitiveTopicsClassification',\n",
    "]\n",
    "\n",
    "try:\n",
    "    tasks = mteb.get_tasks(tasks=tasks_list)\n",
    "    evaluator = mteb.MTEB(tasks=tasks)\n",
    "    results = evaluator.run(\n",
    "        wrapped,\n",
    "        output_folder=\"results/merged_model_mteb\",\n",
    "        eval_splits=[\"test\"],\n",
    "        verbosity=2,\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation completed successfully!\")\n",
    "    \n",
    "    records = []\n",
    "    for r in results:\n",
    "        score = r.get_score()\n",
    "        if 'Clustering' in r.task_name:\n",
    "            task_type = 'Clustering'\n",
    "        elif 'STS' in r.task_name:\n",
    "            task_type = 'STS'\n",
    "        elif r.task_name in ['CEDRClassification', 'SensitiveTopicsClassification']:\n",
    "            task_type = 'MultilabelClassification'\n",
    "        else:\n",
    "            task_type = 'Classification'\n",
    "\n",
    "        \n",
    "        records.append({\n",
    "            \"task\": r.task_name,\n",
    "            \"score\": score,\n",
    "            \"type\": task_type\n",
    "        })\n",
    "        print(f\"{r.task_name}: {score:.4f}\")\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "\n",
    "    print(\"\\n=== Task Scores ===\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    overall_mean = df['score'].mean()\n",
    "    print(f\"\\n=== Overall Mean Score ===\")\n",
    "    print(f\"Overall Mean: {overall_mean:.4f}\")\n",
    "    \n",
    "    type_means = df.groupby('type')['score'].mean().reset_index()\n",
    "    print(f\"\\n=== Scores by Task Type ===\")\n",
    "    print(type_means.to_string(index=False))\n",
    "    \n",
    "    df.to_csv('results/task_scores.csv', index=False)\n",
    "    type_means.to_csv('results/type_means.csv', index=False)\n",
    "    with open('results/overall_mean.txt', 'w') as f:\n",
    "        f.write(f\"{overall_mean:.6f}\")\n",
    "    \n",
    "    print(f\"\\nResults saved to results/ folder\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7548378,
     "sourceId": 11999666,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
